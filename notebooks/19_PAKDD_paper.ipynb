{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab1516a",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42801c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if \"workding_dir\" not in globals():\n",
    "    workding_dir = str(Path.cwd().parent)\n",
    "\n",
    "os.chdir(workding_dir)\n",
    "sys.path.append(workding_dir)\n",
    "print(\"workding dir:\", workding_dir)\n",
    "\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "\n",
    "found_dotenv = find_dotenv(\".env\")\n",
    "\n",
    "if len(found_dotenv) == 0:\n",
    "    found_dotenv = find_dotenv(\".env.example\")\n",
    "print(f\"loading env vars from: {found_dotenv}\")\n",
    "load_dotenv(found_dotenv, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f36348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run cells above before running anything below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220883d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Set global font sizes and style\n",
    "plt.rcParams.update({\n",
    "    'font.size': 14,\n",
    "    'axes.titlesize': 18,\n",
    "    'axes.labelsize': 16,\n",
    "    'xtick.labelsize': 14,\n",
    "    'ytick.labelsize': 14,\n",
    "    'legend.fontsize': 13,\n",
    "    'figure.titlesize': 20,\n",
    "    'font.weight': 'normal',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.titleweight': 'bold'\n",
    "})\n",
    "\n",
    "# Load all CSV files\n",
    "csv_files = {\n",
    "    'amazon': [\n",
    "        'deepseek-amazon_reviews_metrics.csv',\n",
    "        'granite-magistral-amazon_metrics.csv', \n",
    "        'qwen3-amazon_metrics.csv'\n",
    "    ],\n",
    "    'imdb': [\n",
    "        'deepseek-imdb_reviews_results_all_metrics.csv',\n",
    "        'granite-magistral-imdb_metrics.csv',\n",
    "        'qwen3-imdb_metrics.csv'\n",
    "    ],\n",
    "    'goemotions': [\n",
    "        'deepseek-GoEmotions_results_metrics.csv',\n",
    "        'granite-magistral-GoEmotions_metrics.csv',\n",
    "        'qwen3-goemotions_metrics.csv'\n",
    "    ]\n",
    "}\n",
    "\n",
    "def categorize_model(model_name):\n",
    "    \"\"\"Categorize models as reasoning vs base\"\"\"\n",
    "    model_lower = model_name.lower()\n",
    "    \n",
    "    # Reasoning/Thinking models\n",
    "    reasoning_indicators = [\n",
    "        '(t)', 'thinking', 'deepseek-r1', 'deepseek_r1'\n",
    "    ]\n",
    "    \n",
    "    # Check for reasoning models first (more specific)\n",
    "    for indicator in reasoning_indicators:\n",
    "        if indicator in model_lower:\n",
    "            return 'Reasoning/Thinking'\n",
    "    \n",
    "    # Special case for magistral with thinking mode\n",
    "    if 'magistral' in model_lower and '(t)' in model_lower:\n",
    "        return 'Reasoning/Thinking'\n",
    "    \n",
    "    # Everything else is base/non-thinking\n",
    "    return 'Base/Non-thinking'\n",
    "\n",
    "def load_dataset(files, metric_col):\n",
    "    \"\"\"Load and combine data from multiple CSV files\"\"\"\n",
    "    all_data = []\n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.read_csv(f\"results/paper/{file}\")\n",
    "            df['dataset'] = file.split('-')[0].replace('deepseek', '').replace('granite', '').replace('qwen3', '')\n",
    "            all_data.append(df)\n",
    "        except FileNotFoundError:\n",
    "            # Try without the results/paper/ prefix\n",
    "            df = pd.read_csv(file)\n",
    "            df['dataset'] = file.split('-')[0].replace('deepseek', '').replace('granite', '').replace('qwen3', '')\n",
    "            all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Calculate computational cost (inverse of throughput) and best F1\n",
    "    results = []\n",
    "    included_model_shots = []\n",
    "    for _, row in combined_df.iterrows():\n",
    "        model_shots = f\"{row['model']}_{row['shots']}\"\n",
    "        if model_shots in included_model_shots:\n",
    "            print(f\"Skipping duplicate model_shots: {model_shots}\")\n",
    "            continue\n",
    "        included_model_shots.append(model_shots)\n",
    "        \n",
    "        if pd.notna(row[metric_col]) and pd.notna(row['eval_time']): # and row[metric_col] > 0.1:\n",
    "            model = row['model']\n",
    "            if model.lower() in ['deepseek-r1', 'deepseek-v3']:\n",
    "                print(f\"Skipping model {model}\")\n",
    "                continue\n",
    "\n",
    "            model = model.replace('granite', 'Granite').replace('qwen3', 'Qwen3').replace('llama', 'Llama-').replace(':', '-').replace('b', 'B')\n",
    "\n",
    "            results.append({\n",
    "                'model': model,\n",
    "                'shots': row['shots'], \n",
    "                'f1': row[metric_col] * 100,  # Convert to percentage\n",
    "                'cost': row['eval_time'],\n",
    "                'speed': 1 / row['eval_time'], \n",
    "                'model_type': categorize_model(row['model'])\n",
    "            })\n",
    "\n",
    "    print(\"#included_model_shots:\", len(included_model_shots))\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Load data for all three datasets\n",
    "amazon_data = load_dataset(csv_files['amazon'], 'f1_5_level')\n",
    "amazon_data['dataset'] = 'Amazon (5-class)'\n",
    "amazon_data['color'] = 'green'\n",
    "\n",
    "imdb_data = load_dataset(csv_files['imdb'], 'f1')\n",
    "imdb_data['dataset'] = 'IMDB (binary)'\n",
    "imdb_data['color'] = 'blue'\n",
    "\n",
    "goemotions_data = load_dataset(csv_files['goemotions'], 'f1')\n",
    "goemotions_data['dataset'] = 'GoEmotions (27-class)'\n",
    "goemotions_data['color'] = 'red'\n",
    "\n",
    "# Combine all data\n",
    "all_data = pd.concat([amazon_data, imdb_data, goemotions_data], ignore_index=True)\n",
    "\n",
    "# Separate reasoning and base models\n",
    "reasoning_data = all_data[all_data['model_type'] == 'Reasoning/Thinking']\n",
    "base_data = all_data[all_data['model_type'] == 'Base/Non-thinking']\n",
    "\n",
    "# Create the efficiency frontier plot with larger figure size\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Enhanced color palette with better contrast\n",
    "colors = {\n",
    "    'green': '#228B22',    # Forest Green\n",
    "    'blue': '#1E90FF',     # Dodger Blue  \n",
    "    'red': '#DC143C'       # Crimson\n",
    "}\n",
    "\n",
    "# Enhanced marker properties\n",
    "marker_size = 80\n",
    "edge_width = 1.2\n",
    "alpha = 0.8\n",
    "\n",
    "# Create scatter plot for each dataset and model type\n",
    "for dataset, color in [('IMDB (binary)', 'blue'), ('Amazon (5-class)', 'green'), ('GoEmotions (27-class)', 'red')]:\n",
    "    # Base models - circles\n",
    "    base_subset = base_data[base_data['dataset'] == dataset]\n",
    "    if len(base_subset) > 0:\n",
    "        ax.scatter(base_subset['cost'], base_subset['f1'], \n",
    "                  c=colors[color], alpha=alpha, s=marker_size, \n",
    "                  marker='o', edgecolors='white', linewidth=edge_width,\n",
    "                  label=f'{dataset}')\n",
    "    \n",
    "    # Reasoning models - triangles\n",
    "    reasoning_subset = reasoning_data[reasoning_data['dataset'] == dataset]\n",
    "    if len(reasoning_subset) > 0:\n",
    "        ax.scatter(reasoning_subset['cost'], reasoning_subset['f1'], \n",
    "                  c=colors[color], alpha=alpha, s=marker_size,\n",
    "                  marker='^', edgecolors='white', linewidth=edge_width)\n",
    "\n",
    "# Set log scale for x-axis (cost)\n",
    "ax.set_xscale('log')\n",
    "\n",
    "# Enhanced labels and title\n",
    "ax.set_xlabel('Computational Cost (Log-scaled Mean Per-sample Latency in Seconds)', \n",
    "              fontsize=16, fontweight='bold', labelpad=15)\n",
    "ax.set_ylabel('F1 Score (%)', fontsize=16, fontweight='bold', labelpad=15)\n",
    "\n",
    "# Multi-line title for better readability\n",
    "# title_line1 = 'Performance vs. Computational Cost Trade-offs by Model Type'\n",
    "# title_line2 = 'Across All Model Configurations'\n",
    "# ax.set_title(f'{title_line1}\\n{title_line2}', \n",
    "#             fontsize=18, fontweight='bold', pad=25)\n",
    "\n",
    "# Enhanced grid\n",
    "ax.grid(True, alpha=0.4, linestyle='--', linewidth=0.8)\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "# Create enhanced custom legend with better organization\n",
    "legend_elements = []\n",
    "\n",
    "# Dataset legend (top section)\n",
    "legend_elements.append(Line2D([0], [0], color='black', linewidth=2, \n",
    "                             label='Datasets:', linestyle='None'))\n",
    "\n",
    "for dataset, color in [('IMDB (binary)', 'blue'), ('Amazon (5-class)', 'green'), ('GoEmotions (27-class)', 'red')]:\n",
    "    legend_elements.append(Line2D([0], [0], marker='o', color='w', \n",
    "                                 markerfacecolor=colors[color], \n",
    "                                 markersize=10, label=f'  {dataset}', \n",
    "                                 markeredgecolor='white', markeredgewidth=1.2))\n",
    "\n",
    "# Separator\n",
    "legend_elements.append(Line2D([0], [0], color='white', label=''))\n",
    "\n",
    "# Model type legend (bottom section)\n",
    "legend_elements.append(Line2D([0], [0], color='black', linewidth=2,\n",
    "                             label='Model Types:', linestyle='None'))\n",
    "\n",
    "legend_elements.append(Line2D([0], [0], marker='o', color='w', \n",
    "                             markerfacecolor='#666666', \n",
    "                             markersize=10, label='  Base/Non-thinking', \n",
    "                             markeredgecolor='white', markeredgewidth=1.2))\n",
    "legend_elements.append(Line2D([0], [0], marker='^', color='w', \n",
    "                             markerfacecolor='#666666', \n",
    "                             markersize=10, label='  Reasoning/Thinking', \n",
    "                             markeredgecolor='white', markeredgewidth=1.2))\n",
    "\n",
    "# Enhanced legend with better positioning and styling\n",
    "legend = ax.legend(handles=legend_elements, loc='lower right', \n",
    "                  fontsize=13, frameon=True, fancybox=True, \n",
    "                  shadow=True, framealpha=0.95, \n",
    "                  bbox_to_anchor=(0.98, 0.02),\n",
    "                  borderpad=1.2, columnspacing=1.5, handletextpad=0.8)\n",
    "\n",
    "# Enhance legend frame\n",
    "legend.get_frame().set_facecolor('white')\n",
    "legend.get_frame().set_edgecolor('gray')\n",
    "legend.get_frame().set_linewidth(1.5)\n",
    "\n",
    "# Find and annotate key points with enhanced styling\n",
    "amazon_subset = all_data[all_data['dataset'] == 'Amazon (5-class)']\n",
    "imdb_subset = all_data[all_data['dataset'] == 'IMDB (binary)']\n",
    "goemotions_subset = all_data[all_data['dataset'] == 'GoEmotions (27-class)']\n",
    "\n",
    "# Best performance for each dataset\n",
    "amazon_best = amazon_subset.loc[amazon_subset['f1'].idxmax()]\n",
    "imdb_best = imdb_subset.loc[imdb_subset['f1'].idxmax()]\n",
    "goemotions_best = goemotions_subset.loc[goemotions_subset['f1'].idxmax()]\n",
    "\n",
    "# Enhanced annotation styling\n",
    "annotation_fontsize = 12\n",
    "annotation_props = dict(\n",
    "    fontsize=annotation_fontsize, \n",
    "    ha='center', va='center',\n",
    "    bbox=dict(boxstyle='round,pad=0.5', alpha=0.9, edgecolor='black', linewidth=1.5),\n",
    "    arrowprops=dict(arrowstyle='->', lw=2, color='black')\n",
    ")\n",
    "\n",
    "# Annotate key points with better positioning\n",
    "ax.annotate(f'Best Amazon\\n{amazon_best[\"model\"]}\\n{amazon_best[\"f1\"]:.1f}%', \n",
    "           xy=(amazon_best['cost'], amazon_best['f1']), \n",
    "           xytext=(-180, 15), textcoords='offset points',\n",
    "           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightgreen', \n",
    "                    alpha=0.9, edgecolor='darkgreen', linewidth=1.5),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='darkgreen'),\n",
    "           fontsize=annotation_fontsize, ha='center', va='center')\n",
    "\n",
    "ax.annotate(f'Best IMDB\\n{imdb_best[\"model\"]}\\n{imdb_best[\"f1\"]:.1f}%', \n",
    "           xy=(imdb_best['cost'], imdb_best['f1']), \n",
    "           xytext=(80, -60), textcoords='offset points',\n",
    "           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightblue', \n",
    "                    alpha=0.9, edgecolor='darkblue', linewidth=1.5),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='darkblue'),\n",
    "           fontsize=annotation_fontsize, ha='center', va='center')\n",
    "\n",
    "ax.annotate(f'Best GoEmotions\\n{goemotions_best[\"model\"]}\\n{goemotions_best[\"f1\"]:.1f}%', \n",
    "           xy=(goemotions_best['cost'], goemotions_best['f1']), \n",
    "           xytext=(60, 40), textcoords='offset points',\n",
    "           bbox=dict(boxstyle='round,pad=0.5', facecolor='lightcoral', \n",
    "                    alpha=0.9, edgecolor='darkred', linewidth=1.5),\n",
    "           arrowprops=dict(arrowstyle='->', lw=2, color='darkred'),\n",
    "           fontsize=annotation_fontsize, ha='center', va='center')\n",
    "\n",
    "# Add enhanced efficiency frontier lines\n",
    "for dataset, color in [('Amazon (5-class)', 'green'), ('IMDB (binary)', 'blue'), ('GoEmotions (27-class)', 'red')]:\n",
    "    data_subset = all_data[all_data['dataset'] == dataset].copy()\n",
    "    data_subset = data_subset.sort_values('cost')\n",
    "    \n",
    "    # Calculate Pareto frontier (efficiency frontier)\n",
    "    frontier_points = []\n",
    "    max_f1_so_far = 0\n",
    "    \n",
    "    for _, point in data_subset.iterrows():\n",
    "        if point['f1'] > max_f1_so_far:\n",
    "            frontier_points.append(point)\n",
    "            max_f1_so_far = point['f1']\n",
    "    \n",
    "    if frontier_points:\n",
    "        frontier_df = pd.DataFrame(frontier_points)\n",
    "        ax.plot(frontier_df['cost'], frontier_df['f1'], \n",
    "               color=colors[color], linestyle='--', alpha=0.9, \n",
    "               linewidth=3, zorder=10)\n",
    "\n",
    "# Enhanced axis limits and ticks\n",
    "ax.set_ylim(20, 102)\n",
    "ax.set_xlim(left=all_data['cost'].min() * 0.8, right=all_data['cost'].max() * 1.2)\n",
    "\n",
    "# Enhance tick formatting\n",
    "ax.tick_params(axis='both', which='major', labelsize=14, width=1.2, length=6)\n",
    "ax.tick_params(axis='both', which='minor', width=0.8, length=4)\n",
    "\n",
    "# Add subtle background color\n",
    "ax.set_facecolor('#fafafa')\n",
    "\n",
    "# Enhance spines\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(1.2)\n",
    "    spine.set_color('#333333')\n",
    "\n",
    "# Adjust layout to prevent clipping\n",
    "plt.tight_layout(pad=2.0)\n",
    "\n",
    "# Save with high quality\n",
    "plt.savefig('results/paper/pareto_frontier_v2.pdf', \n",
    "           dpi=600, bbox_inches='tight', \n",
    "           facecolor='white', edgecolor='none',\n",
    "           format='pdf')\n",
    "\n",
    "plt.savefig('results/paper/pareto_frontier_v2.png', \n",
    "           dpi=300, bbox_inches='tight', \n",
    "           facecolor='white', edgecolor='none',\n",
    "           format='png')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Enhanced statistical analysis with better formatting\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE EFFICIENCY ANALYSIS REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nüìä DATASET OVERVIEW\")\n",
    "print(f\"   Total configurations analyzed: {len(all_data):,}\")\n",
    "print(f\"   Base/Non-thinking models: {len(base_data):,} ({len(base_data)/len(all_data)*100:.1f}%)\")\n",
    "print(f\"   Reasoning/Thinking models: {len(reasoning_data):,} ({len(reasoning_data)/len(all_data)*100:.1f}%)\")\n",
    "print(f\"   Computational cost range: {all_data['cost'].min():.6f} - {all_data['cost'].max():.6f}s\")\n",
    "print(f\"   Performance range: {all_data['f1'].min():.1f}% - {all_data['f1'].max():.1f}% F1\")\n",
    "\n",
    "print(f\"\\nüìà OVERALL MODEL TYPE COMPARISON\")\n",
    "if len(base_data) > 0:\n",
    "    print(f\"   Base models:\")\n",
    "    print(f\"     ‚Ä¢ Average F1: {base_data['f1'].mean():.1f}% (¬±{base_data['f1'].std():.1f}%)\")\n",
    "    print(f\"     ‚Ä¢ Average Cost: {base_data['cost'].mean():.6f}s (¬±{base_data['cost'].std():.6f}s)\")\n",
    "    print(f\"     ‚Ä¢ Median Cost: {base_data['cost'].median():.6f}s\")\n",
    "\n",
    "if len(reasoning_data) > 0:\n",
    "    print(f\"   Reasoning models:\")\n",
    "    print(f\"     ‚Ä¢ Average F1: {reasoning_data['f1'].mean():.1f}% (¬±{reasoning_data['f1'].std():.1f}%)\")\n",
    "    print(f\"     ‚Ä¢ Average Cost: {reasoning_data['cost'].mean():.6f}s (¬±{reasoning_data['cost'].std():.6f}s)\")\n",
    "    print(f\"     ‚Ä¢ Median Cost: {reasoning_data['cost'].median():.6f}s\")\n",
    "\n",
    "if len(base_data) > 0 and len(reasoning_data) > 0:\n",
    "    cost_ratio = reasoning_data['cost'].mean() / base_data['cost'].mean()\n",
    "    perf_diff = reasoning_data['f1'].mean() - base_data['f1'].mean()\n",
    "    print(f\"\\n   üìä RELATIVE COMPARISON:\")\n",
    "    print(f\"     ‚Ä¢ Cost ratio (Reasoning/Base): {cost_ratio:.1f}√ó slower\")\n",
    "    print(f\"     ‚Ä¢ Performance difference: {perf_diff:+.1f} percentage points\")\n",
    "    print(f\"     ‚Ä¢ Efficiency ratio: {perf_diff/cost_ratio:.2f} F1 points per cost unit\")\n",
    "\n",
    "print(f\"\\nüéØ DATASET-SPECIFIC DETAILED ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dataset in ['Amazon (5-class)', 'IMDB (binary)', 'GoEmotions (27-class)']:\n",
    "    subset = all_data[all_data['dataset'] == dataset]\n",
    "    base_subset = base_data[base_data['dataset'] == dataset]\n",
    "    reasoning_subset = reasoning_data[reasoning_data['dataset'] == dataset]\n",
    "    \n",
    "    print(f\"\\nüìÅ {dataset}:\")\n",
    "    print(f\"   Total configurations: {len(subset):,}\")\n",
    "    print(f\"   Model split: {len(base_subset)} Base, {len(reasoning_subset)} Reasoning\")\n",
    "    print(f\"   F1 range: {subset['f1'].min():.1f}% ‚Üí {subset['f1'].max():.1f}%\")\n",
    "    print(f\"   Cost range: {subset['cost'].min():.6f}s ‚Üí {subset['cost'].max():.6f}s\")\n",
    "    \n",
    "    if len(base_subset) > 0 and len(reasoning_subset) > 0:\n",
    "        base_avg_f1 = base_subset['f1'].mean()\n",
    "        base_avg_cost = base_subset['cost'].mean()\n",
    "        reasoning_avg_f1 = reasoning_subset['f1'].mean()\n",
    "        reasoning_avg_cost = reasoning_subset['cost'].mean()\n",
    "        \n",
    "        print(f\"   üìä Base models: {base_avg_f1:.1f}% F1, {base_avg_cost:.6f}s cost\")\n",
    "        print(f\"   üß† Reasoning models: {reasoning_avg_f1:.1f}% F1, {reasoning_avg_cost:.6f}s cost\")\n",
    "        \n",
    "        perf_advantage = reasoning_avg_f1 - base_avg_f1\n",
    "        cost_ratio = reasoning_avg_cost / base_avg_cost\n",
    "        print(f\"   üí° Reasoning advantage: {perf_advantage:+.1f}% F1 at {cost_ratio:.1f}√ó cost\")\n",
    "    \n",
    "    # Find most efficient high performer\n",
    "    high_performers = subset[subset['f1'] > subset['f1'].quantile(0.9)]\n",
    "    if len(high_performers) > 0:\n",
    "        most_efficient = high_performers.loc[high_performers['cost'].idxmin()]\n",
    "        print(f\"   üèÜ Most efficient top performer:\")\n",
    "        print(f\"       {most_efficient['model']} ({most_efficient['model_type']})\")\n",
    "        print(f\"       {most_efficient['f1']:.1f}% F1, {most_efficient['cost']:.6f}s cost\")\n",
    "\n",
    "print(f\"\\nüöÄ EFFICIENCY FRONTIER ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for dataset in ['Amazon (5-class)', 'IMDB (binary)', 'GoEmotions (27-class)']:\n",
    "    data_subset = all_data[all_data['dataset'] == dataset].copy()\n",
    "    data_subset = data_subset.sort_values('cost')\n",
    "    \n",
    "    # Calculate Pareto frontier\n",
    "    frontier_points = []\n",
    "    max_f1_so_far = 0\n",
    "    \n",
    "    for _, point in data_subset.iterrows():\n",
    "        if point['f1'] > max_f1_so_far:\n",
    "            frontier_points.append(point)\n",
    "            max_f1_so_far = point['f1']\n",
    "    \n",
    "    if frontier_points:\n",
    "        frontier_df = pd.DataFrame(frontier_points)\n",
    "        base_on_frontier = len(frontier_df[frontier_df['model_type'] == 'Base/Non-thinking'])\n",
    "        reasoning_on_frontier = len(frontier_df[frontier_df['model_type'] == 'Reasoning/Thinking'])\n",
    "        \n",
    "        print(f\"\\nüéØ {dataset}:\")\n",
    "        print(f\"   Pareto frontier points: {len(frontier_df)}\")\n",
    "        print(f\"   Base models on frontier: {base_on_frontier} ({base_on_frontier/len(frontier_df)*100:.1f}%)\")\n",
    "        print(f\"   Reasoning models on frontier: {reasoning_on_frontier} ({reasoning_on_frontier/len(frontier_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Show frontier composition\n",
    "        if len(frontier_df) > 0:\n",
    "            print(f\"   üèÖ Frontier models:\")\n",
    "            for _, model in frontier_df.iterrows():\n",
    "                efficiency_score = model['f1'] / (model['cost'] * 1000)  # F1 per millisecond\n",
    "                print(f\"       ‚Ä¢ {model['model']} ({model['model_type'][:4]}): \"\n",
    "                      f\"{model['f1']:.1f}% F1, {model['cost']:.6f}s, \"\n",
    "                      f\"Efficiency: {efficiency_score:.1f}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"üìã KEY RECOMMENDATIONS FOR PRACTITIONERS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Generate recommendations based on analysis\n",
    "recommendations = []\n",
    "\n",
    "# Task complexity recommendations\n",
    "amazon_reasoning_advantage = reasoning_data[reasoning_data['dataset'] == 'Amazon (5-class)']['f1'].mean() - base_data[base_data['dataset'] == 'Amazon (5-class)']['f1'].mean() if len(reasoning_data[reasoning_data['dataset'] == 'Amazon (5-class)']) > 0 and len(base_data[base_data['dataset'] == 'Amazon (5-class)']) > 0 else 0\n",
    "imdb_reasoning_advantage = reasoning_data[reasoning_data['dataset'] == 'IMDB (binary)']['f1'].mean() - base_data[base_data['dataset'] == 'IMDB (binary)']['f1'].mean() if len(reasoning_data[reasoning_data['dataset'] == 'IMDB (binary)']) > 0 and len(base_data[base_data['dataset'] == 'IMDB (binary)']) > 0 else 0\n",
    "goemotions_reasoning_advantage = reasoning_data[reasoning_data['dataset'] == 'GoEmotions (27-class)']['f1'].mean() - base_data[base_data['dataset'] == 'GoEmotions (27-class)']['f1'].mean() if len(reasoning_data[reasoning_data['dataset'] == 'GoEmotions (27-class)']) > 0 and len(base_data[base_data['dataset'] == 'GoEmotions (27-class)']) > 0 else 0\n",
    "\n",
    "if goemotions_reasoning_advantage > 2:\n",
    "    recommendations.append(\"‚úÖ Use reasoning models for complex multi-class emotion tasks (27+ classes)\")\n",
    "if imdb_reasoning_advantage < 0:\n",
    "    recommendations.append(\"‚ùå Avoid reasoning models for simple binary classification tasks\")\n",
    "if amazon_reasoning_advantage < 1:\n",
    "    recommendations.append(\"‚ö†Ô∏è  Exercise caution with reasoning models for moderate complexity tasks\")\n",
    "\n",
    "# Cost efficiency recommendations\n",
    "overall_cost_ratio = reasoning_data['cost'].mean() / base_data['cost'].mean() if len(base_data) > 0 and len(reasoning_data) > 0 else 1\n",
    "if overall_cost_ratio > 5:\n",
    "    recommendations.append(f\"üí∞ Consider {overall_cost_ratio:.1f}√ó computational cost when deploying reasoning models\")\n",
    "\n",
    "# Performance recommendations\n",
    "best_overall = all_data.loc[all_data['f1'].idxmax()]\n",
    "recommendations.append(f\"üèÜ Best overall performer: {best_overall['model']} ({best_overall['f1']:.1f}% F1)\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(f\"\\nüìä Figure saved as:\")\n",
    "print(f\"   ‚Ä¢ pareto_frontier_v2.pdf (High-resolution)\")\n",
    "print(f\"   ‚Ä¢ pareto_frontier.png (Web-ready)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "7 * 2 * (5 + 4 + 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451a79ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "168 * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00e42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "(168 * 3 - 462) / 3 / 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentiment-analysis-with-deepseek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
