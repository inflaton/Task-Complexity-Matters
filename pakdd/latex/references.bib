@article{fu2025scaling,
  title={Scaling Reasoning, Losing Control: Evaluating Instruction Following in Large Reasoning Models},
  author={Fu, Tingchen and Gu, Jiawei and Li, Yafu and Qu, Xiaoye and Cheng, Yu},
  journal={arXiv preprint arXiv:2505.14810},
  year={2025}
}

@article{deepseek2025r1,
  title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning},
  author={{DeepSeek-AI}},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@inproceedings{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny},
  booktitle={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{xu2024survey,
  title={A Survey on Knowledge Distillation of Large Language Models},
  author={Xu, Xiaohan and Li, Ming and Wang, Chongyang and Luo, Tao and Wang, Xinyu and Wang, Jinming},
  journal={arXiv preprint arXiv:2402.13116},
  year={2024}
}

@article{chang2024survey,
  title={Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application},
  author={Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie},
  journal={ACM Transactions on Intelligent Systems and Technology},
  year={2024}
}

@inproceedings{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  booktitle={NIPS Deep Learning and Representation Learning Workshop},
  year={2015}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}

@article{yu2024turtlebench,
  title={TurtleBench: Evaluating Top Language Models via Real-World Yes/No Puzzles},
  author={Yu, Qingchen and Song, Shichao and Fang, Ke and Shi, Yunfeng and Zheng, Zifan and Wang, Hanyu},
  journal={arXiv preprint arXiv:2410.05262},
  year={2024}
}

@inproceedings{mcauley2015image,
  title={Image-based recommendations on styles and substitutes},
  author={McAuley, Julian and Targett, Christopher and Shi, Qinfeng and Van Den Hengel, Anton},
  booktitle={Proceedings of the 38th international ACM SIGIR conference on research and development in information retrieval},
  pages={43--52},
  year={2015}
}

@inproceedings{hsieh2023distilling,
  title={Distilling step-by-step! outperforming larger language models with less training data and smaller model sizes},
  author={Hsieh, Cheng-Yu and Li, Chun-Liang and Yeh, Chih-Kuan and Nakhost, Hootan and Fujii, Yasuhisa and Ratner, Alexander},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={8003--8017},
  year={2023}
}

@article{cantini2024xai,
  title={XAI-driven knowledge distillation of large language models for efficient deployment on low-resource devices},
  author={Cantini, Riccardo and Orsino, Alessio and Talia, Domenico},
  journal={Journal of Big Data},
  volume={11},
  number={63},
  year={2024}
}

@article{prabhakar2024factors,
  title={Deciphering the Factors Influencing the Efficacy of Chain-of-Thought: Probability, Memorization, and Noisy Reasoning},
  author={Prabhakar, Akshara and Khandelwal, Udit and Kembhavi, Aniruddha},
  journal={arXiv preprint arXiv:2407.01687},
  year={2024}
}

@article{mirzadeh2024gsm8k,
  title={GSM8K vs GSM1K: How Reliable Are LLM Benchmarks?},
  author={Mirzadeh, Iman and Alizadeh, Keivan and Mehta, Hooman and Khattak, Oncel Tuzel and Faghri, Fartash and Farajtabar, Mehrdad},
  journal={arXiv preprint arXiv:2407.03398},
  year={2024}
}

@article{wang2024planning,
  title={Planning tokens for complex reasoning: A novel approach to enhancing LLM capabilities},
  author={Wang, Zhuoyuan and Li, Hao and Chen, Jinhua and Liu, Xing and Wang, Yankai and Sun, Maosong},
  journal={arXiv preprint arXiv:2406.12865},
  year={2024}
}

@article{openai2024gpt4,
  title={GPT-4 Technical Report},
  author={{OpenAI}},
  journal={arXiv preprint arXiv:2303.08774},
  year={2024}
}

@misc{anthropic2024claude,
  title={The Claude 3 Model Family: Opus, Sonnet, Haiku},
  author={{Anthropic}},
  year={2024},
  howpublished={\url{https://www.anthropic.com/news/claude-3-family}}
}

@misc{alibaba2025qwen3,
  title={Qwen3 Technical Report},
  author={{Alibaba Cloud}},
  year={2025},
  howpublished={\url{https://qwenlm.github.io/blog/qwen3/}}
}

@article{huang2025explainable,
  author={Huang, Donghao and Wang, Zhaoxia},
  journal={IEEE Intelligent Systems}, 
  title={Explainable Sentiment Analysis With DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning}, 
  year={2025},
  volume={40},
  number={6},
  pages={52-63},
  keywords={Cognition;Sentiment analysis;Reviews;Accuracy;Computational modeling;Few shot learning;Artificial intelligence;Training;Throughput;Motion pictures;Large language models;Few shot learning},
  doi={10.1109/MIS.2025.3614967}
}

@misc{ibm2025granite,
  author       = {{IBM}},
  title        = {Granite 3.3 Model Documentation},
  year         = {2025},
  howpublished = {\url{https://www.ibm.com/granite/docs/models/granite/}},
  note         = {Accessed on July 20, 2025}
}

@article{qwen2025technical,
  author  = {{Qwen Team}},
  title   = {Qwen3 Technical Report},
  journal = {arXiv preprint arXiv:2505.09388},
  year    = {2025}
}

@article{rastogi2025magistral,
  author  = {Rastogi, Abhinav and Jiang, Albert Q. and Lo, Andy and Berrada, Gabrielle and Lample, Guillaume and Rute, Jason and Barmentlo, Joep and Yadav, Karmesh and Khandelwal, Kartik and Chandu, Khyathi Raghavi and Blier, L{\'e}onard and Saulnier, Lucile and Dinot, Matthieu and Darrin, Maxime and Gupta, Neha and Soletskyi, Roman and Vaze, Sagar and Le Scao, Teven and Wang, Yihan},
  title   = {Magistral: Transparent Reasoning via Reinforcement Learning},
  journal = {arXiv preprint arXiv:2506.10910},
  year    = {2025}
}

@inproceedings{zhang2024sentiment,
  author    = {Zhang, A. and Li, B. and Wang, C. and Liu, D. and Chen, E.},
  title     = {Sentiment Analysis in the Era of Large Language Models: A Reality Check},
  booktitle = {Proceedings of NAACL 2024},
  year      = {2024}
}

@inproceedings{demszky2020goemotions,
  title={GoEmotions: A Dataset of Fine-Grained Emotions},
  author={Demszky, Dorottya and Movshovitz-Attias, Dana and Ko, Jeongwoo and Cowen, Alan and Nemade, Gaurav and Ravi, Sujith},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={4040--4054},
  year={2020}
}

@inproceedings{mukherjee2019utilization,
  title={Utilization of oversampling for multi-class sentiment analysis on Amazon review dataset},
  author={Mukherjee, A. and Mukhopadhyay, S. and Panigrahi, P.K. and Goswami, S.},
  booktitle={IEEE 10th International Conference on Awareness Science and Technology (iCAST)},
  pages={1--6},
  year={2019},
  publisher={IEEE}
}

@article{steinke2022sentiment,
  title={Sentiment analysis of online movie reviews using machine learning},
  author={Steinke, I. and Wier, J. and Simon, L. and Seetan, R.},
  journal={International Journal of Advanced Computer Science and Applications},
  volume={13},
  number={9},
  pages={618--624},
  year={2022}
}

% New references based on review recommendations

@book{kahneman2011thinking,
  title={Thinking, Fast and Slow},
  author={Kahneman, Daniel},
  year={2011},
  publisher={Farrar, Straus and Giroux}
}

@article{li2025financial,
  title={Reasoning or Overthinking: Evaluating Large Language Models on Financial Sentiment Analysis},
  author={Li, J. and Chen, M. and Wang, Q. and Zhang, Y.},
  journal={arXiv preprint arXiv:2506.04574},
  year={2025}
}

@article{zhang2025targeted,
  title={Targeted Distillation for Sentiment Analysis},
  author={Zhang, L. and Liu, P. and Wang, H. and Chen, S.},
  journal={arXiv preprint arXiv:2503.03225},
  year={2025}
}

@book{picard1997affective,
  title={Affective Computing},
  author={Picard, Rosalind W.},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{yu2025aspect,
  title={Aspect-Based Sentiment Analysis with Syntax-Enhanced Chain Reasoning},
  author={Yu, K. and Zhang, W. and Liu, M. and Chen, X.},
  booktitle={Proceedings of COLING 2025},
  pages={2485--2497},
  year={2025}
}

@article{chen2025interpretability,
  title={Mechanistic Interpretability of Reasoning in Large Language Models},
  author={Chen, Y. and Wang, L. and Zhang, M. and Liu, J.},
  journal={arXiv preprint arXiv:2503.03730},
  year={2025}
}

@article{wang2025emotional,
  title={Recent Advancement of Emotion Cognition in Large Language Models},
  author={Wang, S. and Li, H. and Chen, R. and Zhang, K.},
  journal={arXiv preprint arXiv:2409.13354},
  year={2025}
}

@article{wang2025review,
  title={A review of Chinese sentiment analysis: subjects, methods, and trends},
  author={Wang, Zhaoxia and Huang, Donghao and Cui, Jingfeng and Zhang, Xinyue and Ho, Seng-Beng and Cambria, Erik},
  journal={Artificial Intelligence Review},
  volume={58},
  number={3},
  pages={75},
  year={2025},
  publisher={Springer}
}